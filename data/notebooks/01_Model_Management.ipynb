{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e116dd52",
   "metadata": {},
   "source": [
    "# AI Beast - Model Management\n",
    "\n",
    "This notebook demonstrates model management operations with AI Beast.\n",
    "\n",
    "## Features Covered\n",
    "- Scanning local models\n",
    "- Listing Ollama models\n",
    "- Storage analysis\n",
    "- Model metadata inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90dcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add AI Beast workspace to path\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan/ai_beast')\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84801fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AI Beast context\n",
    "from modules.container import AppContext\n",
    "\n",
    "# Create context pointing to workspace\n",
    "context = AppContext(\n",
    "    base_dir=Path('/home/jovyan/ai_beast'),\n",
    "    models_dir=Path('/home/jovyan/models'),\n",
    "    data_dir=Path('/home/jovyan/work'),\n",
    "    ports={},\n",
    "    features={},\n",
    ")\n",
    "\n",
    "print(f\"Base Dir: {context.base_dir}\")\n",
    "print(f\"Models Dir: {context.models_dir}\")\n",
    "print(f\"Data Dir: {context.data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07961b4a",
   "metadata": {},
   "source": [
    "## Scan Local Models\n",
    "\n",
    "Scan all local model directories for GGUF, safetensors, and other model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889228e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.llm.manager_async import AsyncLLMManager\n",
    "\n",
    "async def scan_models():\n",
    "    \"\"\"Scan and list all local models.\"\"\"\n",
    "    async with AsyncLLMManager(context) as manager:\n",
    "        models = await manager.scan_local_models_async(force=True)\n",
    "        return models\n",
    "\n",
    "# Run scan\n",
    "models = await scan_models()\n",
    "\n",
    "print(f\"Found {len(models)} models:\\n\")\n",
    "for model in models:\n",
    "    print(f\"üì¶ {model.name}\")\n",
    "    print(f\"   Location: {model.location.value}\")\n",
    "    print(f\"   Size: {model.size_human}\")\n",
    "    print(f\"   Type: {model.model_type}\")\n",
    "    if model.quantization:\n",
    "        print(f\"   Quantization: {model.quantization}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248bb503",
   "metadata": {},
   "source": [
    "## Ollama Models\n",
    "\n",
    "List models available through the Ollama API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c154ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ollama import OllamaClient\n",
    "\n",
    "async def list_ollama_models():\n",
    "    \"\"\"List Ollama models.\"\"\"\n",
    "    # Connect to Ollama on host\n",
    "    async with OllamaClient(base_url=\"http://host.docker.internal:11434\") as client:\n",
    "        return await client.list_models()\n",
    "\n",
    "try:\n",
    "    ollama_models = await list_ollama_models()\n",
    "    print(f\"Ollama models: {len(ollama_models)}\\n\")\n",
    "    for model in ollama_models:\n",
    "        print(f\"ü§ñ {model['name']}\")\n",
    "        if 'size' in model:\n",
    "            size_gb = model['size'] / (1024**3)\n",
    "            print(f\"   Size: {size_gb:.2f} GB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not connect to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running on the host machine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44408a7b",
   "metadata": {},
   "source": [
    "## Storage Analysis\n",
    "\n",
    "Analyze model storage usage with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert models to DataFrame\n",
    "if models:\n",
    "    df = pd.DataFrame([{\n",
    "        'name': m.name,\n",
    "        'location': m.location.value,\n",
    "        'size_bytes': m.size_bytes,\n",
    "        'size_gb': m.size_bytes / (1024**3),\n",
    "        'model_type': m.model_type,\n",
    "        'quantization': m.quantization or 'N/A',\n",
    "    } for m in models])\n",
    "    \n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No models found to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30606231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage by location visualization\n",
    "if models:\n",
    "    storage_by_location = df.groupby('location')['size_gb'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart\n",
    "    storage_by_location.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('Model Storage by Location')\n",
    "    axes[0].set_ylabel('Size (GB)')\n",
    "    axes[0].set_xlabel('Location')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    storage_by_location.plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
    "    axes[1].set_title('Storage Distribution')\n",
    "    axes[1].set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Total storage: {df['size_gb'].sum():.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage by model type\n",
    "if models and 'model_type' in df.columns:\n",
    "    storage_by_type = df.groupby('model_type')['size_gb'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    storage_by_type.plot(kind='barh', color='coral')\n",
    "    plt.title('Storage by Model Type')\n",
    "    plt.xlabel('Size (GB)')\n",
    "    plt.ylabel('Model Type')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8518e8c5",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "\n",
    "Inspect detailed metadata for a specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ae27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the largest model for inspection\n",
    "if models:\n",
    "    largest_model = max(models, key=lambda m: m.size_bytes)\n",
    "    print(f\"Largest Model: {largest_model.name}\")\n",
    "    print(f\"=\"*50)\n",
    "    pprint(largest_model.to_dict())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
