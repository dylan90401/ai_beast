{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a57650",
   "metadata": {},
   "source": [
    "# AI Beast - RAG Analysis\n",
    "\n",
    "This notebook demonstrates RAG (Retrieval-Augmented Generation) operations.\n",
    "\n",
    "## Features Covered\n",
    "- Connecting to Qdrant vector store\n",
    "- Collection analysis\n",
    "- Document embedding inspection\n",
    "- Semantic search testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f26ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup path and imports\n",
    "import sys\n",
    "sys.path.insert(0, '/home/jovyan/ai_beast')\n",
    "\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43ad67",
   "metadata": {},
   "source": [
    "## Connect to Qdrant\n",
    "\n",
    "Connect to the Qdrant vector database for RAG operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Connect to Qdrant on host\n",
    "QDRANT_URL = \"http://host.docker.internal:6333\"\n",
    "\n",
    "try:\n",
    "    client = QdrantClient(url=QDRANT_URL)\n",
    "    collections = client.get_collections()\n",
    "    print(f\"âœ… Connected to Qdrant\")\n",
    "    print(f\"ðŸ“š Collections: {len(collections.collections)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not connect to Qdrant: {e}\")\n",
    "    print(\"Make sure Qdrant is running (beast start qdrant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840044dc",
   "metadata": {},
   "source": [
    "## List Collections\n",
    "\n",
    "View all available vector collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f58550",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'client' in dir():\n",
    "    collections = client.get_collections().collections\n",
    "    \n",
    "    if collections:\n",
    "        print(\"Available Collections:\\n\")\n",
    "        for coll in collections:\n",
    "            info = client.get_collection(coll.name)\n",
    "            print(f\"ðŸ“ {coll.name}\")\n",
    "            print(f\"   Points: {info.points_count:,}\")\n",
    "            print(f\"   Vectors: {info.vectors_count:,}\")\n",
    "            print(f\"   Status: {info.status}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No collections found. Run 'beast rag ingest' to create one.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369d0fb",
   "metadata": {},
   "source": [
    "## Collection Analysis\n",
    "\n",
    "Analyze a specific collection's contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c49db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select collection to analyze\n",
    "COLLECTION_NAME = \"ai_beast_docs\"  # Change this to your collection\n",
    "\n",
    "if 'client' in dir():\n",
    "    try:\n",
    "        collection_info = client.get_collection(COLLECTION_NAME)\n",
    "        \n",
    "        print(f\"Collection: {COLLECTION_NAME}\")\n",
    "        print(f\"=\"*50)\n",
    "        print(f\"Points count: {collection_info.points_count:,}\")\n",
    "        print(f\"Vectors count: {collection_info.vectors_count:,}\")\n",
    "        print(f\"Status: {collection_info.status}\")\n",
    "        \n",
    "        if collection_info.config:\n",
    "            config = collection_info.config\n",
    "            if hasattr(config, 'params'):\n",
    "                params = config.params\n",
    "                if hasattr(params, 'vectors'):\n",
    "                    vectors_config = params.vectors\n",
    "                    print(f\"\\nVector Configuration:\")\n",
    "                    if hasattr(vectors_config, 'size'):\n",
    "                        print(f\"  Dimension: {vectors_config.size}\")\n",
    "                    if hasattr(vectors_config, 'distance'):\n",
    "                        print(f\"  Distance: {vectors_config.distance}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Collection '{COLLECTION_NAME}' not found: {e}\")\n",
    "        print(\"\\nAvailable collections:\")\n",
    "        for c in client.get_collections().collections:\n",
    "            print(f\"  - {c.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d4e96",
   "metadata": {},
   "source": [
    "## Sample Points\n",
    "\n",
    "View sample documents from the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9969351",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'client' in dir():\n",
    "    try:\n",
    "        # Get sample points\n",
    "        points, _ = client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            limit=5,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        \n",
    "        print(f\"Sample Documents ({len(points)} points):\\n\")\n",
    "        for i, point in enumerate(points, 1):\n",
    "            print(f\"--- Document {i} (ID: {point.id}) ---\")\n",
    "            payload = point.payload or {}\n",
    "            \n",
    "            if 'source' in payload:\n",
    "                print(f\"Source: {payload['source']}\")\n",
    "            if 'text' in payload:\n",
    "                text = payload['text'][:200] + '...' if len(payload.get('text', '')) > 200 else payload.get('text', '')\n",
    "                print(f\"Text: {text}\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching points: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c58cdb",
   "metadata": {},
   "source": [
    "## Semantic Search Test\n",
    "\n",
    "Test semantic search with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f189627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding using Ollama\n",
    "from modules.ollama import OllamaClient\n",
    "\n",
    "async def get_embedding(text: str, model: str = \"nomic-embed-text\"):\n",
    "    \"\"\"Get embedding from Ollama.\"\"\"\n",
    "    async with OllamaClient(base_url=\"http://host.docker.internal:11434\") as client:\n",
    "        return await client.embed(text, model=model)\n",
    "\n",
    "# Test query\n",
    "QUERY = \"How do I download a model?\"\n",
    "\n",
    "try:\n",
    "    embedding = await get_embedding(QUERY)\n",
    "    print(f\"âœ… Generated embedding (dim: {len(embedding)})\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not generate embedding: {e}\")\n",
    "    embedding = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f977bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform semantic search\n",
    "if 'client' in dir() and embedding:\n",
    "    try:\n",
    "        results = client.search(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            query_vector=embedding,\n",
    "            limit=5,\n",
    "            with_payload=True,\n",
    "        )\n",
    "        \n",
    "        print(f\"Search Results for: '{QUERY}'\\n\")\n",
    "        print(f\"=\"*60)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Score: {result.score:.4f}\")\n",
    "            payload = result.payload or {}\n",
    "            if 'source' in payload:\n",
    "                print(f\"   Source: {payload['source']}\")\n",
    "            if 'text' in payload:\n",
    "                text = payload['text'][:300] + '...' if len(payload.get('text', '')) > 300 else payload.get('text', '')\n",
    "                print(f\"   Text: {text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Search failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f65db6",
   "metadata": {},
   "source": [
    "## Embedding Analysis\n",
    "\n",
    "Visualize embedding distributions using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7632c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "if 'client' in dir():\n",
    "    try:\n",
    "        # Get points with vectors\n",
    "        points, _ = client.scroll(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            limit=100,\n",
    "            with_payload=True,\n",
    "            with_vectors=True,\n",
    "        )\n",
    "        \n",
    "        if points:\n",
    "            # Extract vectors\n",
    "            vectors = np.array([p.vector for p in points if p.vector is not None])\n",
    "            \n",
    "            if len(vectors) > 2:\n",
    "                # PCA reduction to 2D\n",
    "                pca = PCA(n_components=2)\n",
    "                vectors_2d = pca.fit_transform(vectors)\n",
    "                \n",
    "                # Plot\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.6, c='steelblue')\n",
    "                plt.title(f'Document Embeddings (PCA) - {COLLECTION_NAME}')\n",
    "                plt.xlabel('PC1')\n",
    "                plt.ylabel('PC2')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"Visualized {len(vectors)} embeddings\")\n",
    "                print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "            else:\n",
    "                print(f\"Need more than 2 vectors for PCA (got {len(vectors)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing embeddings: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
