{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2155f579",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Getting Started with AI Beast\n",
    "\n",
    "Welcome to AI Beast! This tutorial will guide you through the basics of using AI Beast for local AI development.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Check system status\n",
    "2. Connect to Ollama\n",
    "3. Generate your first response\n",
    "4. Stream responses\n",
    "5. Use different models\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AI Beast installed and running\n",
    "- At least one model pulled (e.g., `llama3.2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9211ca",
   "metadata": {},
   "source": [
    "## 1. Check System Status\n",
    "\n",
    "First, let's make sure everything is running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd187c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check AI Beast status using CLI\n",
    "!beast status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run health checks\n",
    "!beast health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74ea49",
   "metadata": {},
   "source": [
    "## 2. Connect to Ollama\n",
    "\n",
    "Let's connect to Ollama and list available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from modules.ollama.client import OllamaClient\n",
    "\n",
    "# Initialize client\n",
    "client = OllamaClient()\n",
    "\n",
    "# Check connection\n",
    "print(\"Connected to Ollama!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models\n",
    "models = await client.list_models()\n",
    "\n",
    "print(\"Available Models:\")\n",
    "for model in models:\n",
    "    print(f\"  - {model.name} ({model.size / 1e9:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d80995",
   "metadata": {},
   "source": [
    "## 3. Generate Your First Response\n",
    "\n",
    "Let's generate a simple response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation\n",
    "response = await client.generate(\n",
    "    model=\"llama3.2\",  # Change this to your model\n",
    "    prompt=\"Explain what a neural network is in one sentence.\",\n",
    ")\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation with options\n",
    "response = await client.generate(\n",
    "    model=\"llama3.2\",\n",
    "    prompt=\"Write a haiku about programming.\",\n",
    "    options={\n",
    "        \"temperature\": 0.9,  # More creative\n",
    "        \"top_p\": 0.95,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1e3a7",
   "metadata": {},
   "source": [
    "## 4. Stream Responses\n",
    "\n",
    "For longer responses, streaming provides a better experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3dd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming generation\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "async for chunk in client.generate_stream(\n",
    "    model=\"llama3.2\",\n",
    "    prompt=\"Tell me a short story about a robot learning to paint.\",\n",
    "):\n",
    "    print(chunk.response, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af551b43",
   "metadata": {},
   "source": [
    "## 5. Chat Conversation\n",
    "\n",
    "Maintain context across multiple messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with message history\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "]\n",
    "\n",
    "response = await client.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5920247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response.message.content})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a simple code example.\"})\n",
    "\n",
    "response = await client.chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(\"Assistant:\", response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c8f6f",
   "metadata": {},
   "source": [
    "## 6. Try Different Models\n",
    "\n",
    "Different models have different strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses from different models\n",
    "prompt = \"What is 15 * 23?\"\n",
    "\n",
    "# Try with available models\n",
    "test_models = [\"llama3.2\", \"phi3:mini\"]  # Adjust based on what you have\n",
    "\n",
    "for model_name in test_models:\n",
    "    try:\n",
    "        response = await client.generate(\n",
    "            model=model_name,\n",
    "            prompt=prompt,\n",
    "        )\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(response.response.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model_name}: Not available ({e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a480bc7",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You've learned the basics of AI Beast.\n",
    "\n",
    "Continue with:\n",
    "- **[02_chat_with_models.ipynb](02_chat_with_models.ipynb)** - Build interactive chat applications\n",
    "- **[03_rag_basics.ipynb](03_rag_basics.ipynb)** - Create document Q&A systems\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [AI Beast Documentation](https://ai-beast.readthedocs.io/)\n",
    "- [Ollama Model Library](https://ollama.ai/library)\n",
    "- [GitHub Repository](https://github.com/dylan90401/ai_beast)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
