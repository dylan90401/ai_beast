{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18c023b1",
   "metadata": {},
   "source": [
    "# ðŸ“š RAG Basics: Document Q&A with AI Beast\n",
    "\n",
    "Learn to build Retrieval-Augmented Generation (RAG) applications that answer questions from your documents.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Understand RAG architecture\n",
    "2. Load and chunk documents\n",
    "3. Create embeddings\n",
    "4. Query with context retrieval\n",
    "5. Build a simple Q&A system\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed previous tutorials\n",
    "- Qdrant running (`beast up qdrant`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ba9d4",
   "metadata": {},
   "source": [
    "## 1. Understanding RAG\n",
    "\n",
    "RAG enhances LLMs by providing relevant context from your documents.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Question  â”‚â”€â”€â”€â”€â–¶â”‚  Retrieval  â”‚â”€â”€â”€â”€â–¶â”‚     LLM     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚                    â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Vector    â”‚      â”‚  Answer   â”‚\n",
    "                    â”‚   Store     â”‚      â”‚  + Source â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Ground responses in your data\n",
    "- Reduce hallucinations\n",
    "- Keep knowledge up-to-date\n",
    "- Maintain source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae721d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from modules.rag import RAGPipeline, Document, ChunkingStrategy\n",
    "from modules.llm import LLMClient\n",
    "\n",
    "# Initialize components\n",
    "llm = LLMClient()\n",
    "print(\"LLM client ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a110d",
   "metadata": {},
   "source": [
    "## 2. Loading Documents\n",
    "\n",
    "First, let's load some documents to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee35f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents (in real use, load from files)\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        content=\"\"\"AI Beast is a local AI infrastructure manager that helps developers\n",
    "        run AI models on their own hardware. It supports multiple providers including\n",
    "        Ollama for local models, OpenAI, and Anthropic for cloud models. The system\n",
    "        uses Docker containers to manage services and provides a unified API for\n",
    "        interacting with different AI providers.\"\"\",\n",
    "        metadata={\"source\": \"overview.md\", \"section\": \"introduction\"}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"To install AI Beast, clone the repository and run the bootstrap\n",
    "        script. The minimum requirements are 16GB RAM and 50GB disk space. For GPU\n",
    "        acceleration, NVIDIA GPUs with CUDA support are recommended. Apple Silicon\n",
    "        Macs can use Metal for acceleration through Ollama.\"\"\",\n",
    "        metadata={\"source\": \"installation.md\", \"section\": \"requirements\"}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"The RAG module allows you to build document Q&A applications.\n",
    "        It uses Qdrant as the vector database for storing embeddings. Documents\n",
    "        are chunked, embedded using local models, and stored for semantic search.\n",
    "        The pipeline supports multiple chunking strategies including fixed-size,\n",
    "        semantic, and sentence-based chunking.\"\"\",\n",
    "        metadata={\"source\": \"rag_guide.md\", \"section\": \"overview\"}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"Ollama is a tool for running large language models locally.\n",
    "        It supports models like Llama 3, Mistral, Phi, and many others. Ollama\n",
    "        handles model downloading, memory management, and provides a simple API.\n",
    "        In AI Beast, Ollama is the default local provider for both chat and\n",
    "        embedding models.\"\"\",\n",
    "        metadata={\"source\": \"ollama_guide.md\", \"section\": \"introduction\"}\n",
    "    ),\n",
    "    Document(\n",
    "        content=\"\"\"To configure AI Beast, edit the config/ai-beast.env file.\n",
    "        Key settings include OLLAMA_HOST for the Ollama endpoint, DEFAULT_MODEL\n",
    "        for the chat model, and EMBEDDING_MODEL for the embedding model. You can\n",
    "        also set API keys for cloud providers like OPENAI_API_KEY and\n",
    "        ANTHROPIC_API_KEY.\"\"\",\n",
    "        metadata={\"source\": \"configuration.md\", \"section\": \"environment\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(sample_docs)} documents\")\n",
    "for doc in sample_docs:\n",
    "    print(f\"  - {doc.metadata['source']}: {len(doc.content)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d6899",
   "metadata": {},
   "source": [
    "## 3. Chunking Documents\n",
    "\n",
    "Large documents need to be split into manageable chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.rag.chunker import Chunker\n",
    "\n",
    "# Create chunker with different strategies\n",
    "chunker = Chunker(\n",
    "    strategy=ChunkingStrategy.FIXED_SIZE,\n",
    "    chunk_size=200,      # Characters per chunk\n",
    "    chunk_overlap=50,    # Overlap between chunks\n",
    ")\n",
    "\n",
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "for doc in sample_docs:\n",
    "    chunks = chunker.chunk(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "    print(f\"{doc.metadata['source']}: {len(chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b38673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a sample chunk\n",
    "print(\"Sample chunk:\")\n",
    "print(\"-\" * 50)\n",
    "print(all_chunks[0].content)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Metadata: {all_chunks[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa940f",
   "metadata": {},
   "source": [
    "## 4. Creating Embeddings\n",
    "\n",
    "Convert text chunks into vector embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecc472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.rag.embeddings import EmbeddingClient\n",
    "\n",
    "# Initialize embedding client\n",
    "embedder = EmbeddingClient(\n",
    "    model=\"nomic-embed-text\"  # Or your preferred embedding model\n",
    ")\n",
    "\n",
    "# Generate embeddings for chunks\n",
    "texts = [chunk.content for chunk in all_chunks]\n",
    "embeddings = await embedder.embed_batch(texts)\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dd441d",
   "metadata": {},
   "source": [
    "## 5. Vector Store\n",
    "\n",
    "Store embeddings in Qdrant for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f965a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.rag.vectorstore import QdrantStore\n",
    "\n",
    "# Initialize vector store\n",
    "vectorstore = QdrantStore(\n",
    "    collection_name=\"tutorial_docs\",\n",
    "    embedding_dim=len(embeddings[0]),\n",
    ")\n",
    "\n",
    "# Create collection (if needed)\n",
    "await vectorstore.create_collection()\n",
    "\n",
    "# Add documents with embeddings\n",
    "await vectorstore.add_documents(\n",
    "    documents=all_chunks,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"Stored {len(all_chunks)} chunks in Qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19889bcd",
   "metadata": {},
   "source": [
    "## 6. Semantic Search\n",
    "\n",
    "Search for relevant documents using natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search function\n",
    "async def search(query: str, top_k: int = 3):\n",
    "    \"\"\"Search for relevant documents.\"\"\"\n",
    "    # Embed the query\n",
    "    query_embedding = await embedder.embed(query)\n",
    "    \n",
    "    # Search vector store\n",
    "    results = await vectorstore.search(\n",
    "        query_embedding=query_embedding,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search\n",
    "query = \"How do I install AI Beast?\"\n",
    "results = await search(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Score: {result.score:.4f}\")\n",
    "    print(f\"   Source: {result.document.metadata.get('source', 'unknown')}\")\n",
    "    print(f\"   Content: {result.document.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858d362",
   "metadata": {},
   "source": [
    "## 7. Building the Q&A System\n",
    "\n",
    "Combine retrieval with LLM for document Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9892f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_question(question: str, top_k: int = 3) -> dict:\n",
    "    \"\"\"Answer a question using RAG.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve relevant context\n",
    "    results = await search(question, top_k=top_k)\n",
    "    \n",
    "    # 2. Build context from results\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    for result in results:\n",
    "        context_parts.append(result.document.content)\n",
    "        source = result.document.metadata.get('source', 'unknown')\n",
    "        if source not in sources:\n",
    "            sources.append(source)\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Create prompt with context\n",
    "    prompt = f\"\"\"Answer the question based on the context below. \n",
    "If the answer is not in the context, say \"I don't have information about that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # 4. Generate answer\n",
    "    response = await llm.chat(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.3},\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"answer\": response.content,\n",
    "        \"sources\": sources,\n",
    "        \"num_chunks_used\": len(results),\n",
    "    }\n",
    "\n",
    "# Test the Q&A system\n",
    "result = await answer_question(\"What are the system requirements for AI Beast?\")\n",
    "\n",
    "print(f\"Q: {result['question']}\\n\")\n",
    "print(f\"A: {result['answer']}\\n\")\n",
    "print(f\"Sources: {', '.join(result['sources'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test more questions\n",
    "questions = [\n",
    "    \"What is Ollama?\",\n",
    "    \"How do I configure AI Beast?\",\n",
    "    \"What chunking strategies are available?\",\n",
    "    \"What is the capital of France?\",  # Not in docs\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"=\"*60)\n",
    "    result = await answer_question(q)\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer'][:300]}...\" if len(result['answer']) > 300 else f\"A: {result['answer']}\")\n",
    "    print(f\"Sources: {', '.join(result['sources'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f756f",
   "metadata": {},
   "source": [
    "## 8. Using the RAG Pipeline\n",
    "\n",
    "AI Beast provides a high-level RAG pipeline that handles all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.rag import RAGPipeline, RAGConfig\n",
    "\n",
    "# Initialize pipeline with config\n",
    "rag = RAGPipeline(\n",
    "    config=RAGConfig(\n",
    "        collection_name=\"tutorial_pipeline\",\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=50,\n",
    "        top_k=3,\n",
    "        embedding_model=\"nomic-embed-text\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ingest documents\n",
    "await rag.ingest(sample_docs)\n",
    "print(\"Documents ingested!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3383e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query using pipeline\n",
    "result = await rag.query(\"What is AI Beast and what providers does it support?\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result.answer)\n",
    "print(f\"\\nSources: {result.sources}\")\n",
    "print(f\"Confidence: {result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bb888",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Excellent! You've built a document Q&A system with RAG.\n",
    "\n",
    "Continue with:\n",
    "- **[04_advanced_prompting.ipynb](04_advanced_prompting.ipynb)** - Master prompt engineering\n",
    "- **[05_building_agents.ipynb](05_building_agents.ipynb)** - Create AI agents with tools\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Load your own documents (PDFs, markdown, etc.)\n",
    "2. Experiment with different chunking strategies\n",
    "3. Add metadata filtering to search\n",
    "4. Build a chat interface with persistent memory\n",
    "\n",
    "## Tips\n",
    "\n",
    "- **Chunk size matters**: Smaller chunks = more precise retrieval, larger = more context\n",
    "- **Overlap helps**: Prevents losing information at chunk boundaries\n",
    "- **Metadata is valuable**: Use it for filtering and source attribution\n",
    "- **Embeddings choice**: Different models have different strengths"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
