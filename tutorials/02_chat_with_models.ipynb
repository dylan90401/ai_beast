{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d501f19",
   "metadata": {},
   "source": [
    "# ðŸ’¬ Building Chat Applications with AI Beast\n",
    "\n",
    "This tutorial covers building interactive chat applications using AI Beast's LLM module.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Use the unified LLM interface\n",
    "2. Manage conversation history\n",
    "3. Build a chat loop\n",
    "4. Add system prompts\n",
    "5. Handle multiple providers\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed [01_getting_started.ipynb](01_getting_started.ipynb)\n",
    "- At least one LLM model available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002b95f",
   "metadata": {},
   "source": [
    "## 1. The Unified LLM Interface\n",
    "\n",
    "AI Beast provides a unified interface for different LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from modules.llm import LLMClient\n",
    "\n",
    "# Initialize with default provider (Ollama)\n",
    "llm = LLMClient()\n",
    "print(f\"Using provider: {llm.provider}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbe092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat\n",
    "response = await llm.chat(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Who are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c4454",
   "metadata": {},
   "source": [
    "## 2. Managing Conversation History\n",
    "\n",
    "Keep track of conversation context for multi-turn chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed244ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    \"\"\"Manages chat conversation history.\"\"\"\n",
    "    \n",
    "    system_prompt: str = \"You are a helpful AI assistant.\"\n",
    "    messages: List[Dict[str, str]] = field(default_factory=list)\n",
    "    max_history: int = 20\n",
    "    \n",
    "    def add_user(self, content: str) -> None:\n",
    "        \"\"\"Add a user message.\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "        self._trim_history()\n",
    "    \n",
    "    def add_assistant(self, content: str) -> None:\n",
    "        \"\"\"Add an assistant message.\"\"\"\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "        self._trim_history()\n",
    "    \n",
    "    def get_messages(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get all messages including system prompt.\"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            *self.messages\n",
    "        ]\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear conversation history.\"\"\"\n",
    "        self.messages.clear()\n",
    "    \n",
    "    def _trim_history(self) -> None:\n",
    "        \"\"\"Keep only recent messages.\"\"\"\n",
    "        if len(self.messages) > self.max_history:\n",
    "            self.messages = self.messages[-self.max_history:]\n",
    "\n",
    "# Create a conversation\n",
    "convo = Conversation(system_prompt=\"You are a friendly Python tutor.\")\n",
    "print(\"Conversation manager created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the conversation\n",
    "async def chat(user_input: str) -> str:\n",
    "    \"\"\"Send a message and get response.\"\"\"\n",
    "    convo.add_user(user_input)\n",
    "    \n",
    "    response = await llm.chat(\n",
    "        messages=convo.get_messages()\n",
    "    )\n",
    "    \n",
    "    convo.add_assistant(response.content)\n",
    "    return response.content\n",
    "\n",
    "# Test multi-turn conversation\n",
    "print(\"User: What is a list in Python?\")\n",
    "response = await chat(\"What is a list in Python?\")\n",
    "print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "print(\"User: How do I add items to it?\")\n",
    "response = await chat(\"How do I add items to it?\")  # \"it\" refers to list from context\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b96c3",
   "metadata": {},
   "source": [
    "## 3. Interactive Chat Loop\n",
    "\n",
    "Build a reusable chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_chat(\n",
    "    system_prompt: str = \"You are a helpful assistant.\",\n",
    "    model: str = \"llama3.2\",\n",
    "    max_turns: int = 10,\n",
    "):\n",
    "    \"\"\"Run an interactive chat session.\"\"\"\n",
    "    \n",
    "    convo = Conversation(system_prompt=system_prompt)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Chat session started (model: {model})\")\n",
    "    print(\"Type 'quit' to exit, 'clear' to reset\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    for turn in range(max_turns):\n",
    "        # Get user input\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'clear':\n",
    "            convo.clear()\n",
    "            print(\"\\n[Conversation cleared]\\n\")\n",
    "            continue\n",
    "        \n",
    "        convo.add_user(user_input)\n",
    "        \n",
    "        # Stream response\n",
    "        print(\"\\nAssistant: \", end=\"\", flush=True)\n",
    "        \n",
    "        full_response = \"\"\n",
    "        async for chunk in llm.chat_stream(\n",
    "            messages=convo.get_messages(),\n",
    "            model=model,\n",
    "        ):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "            full_response += chunk.content\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        convo.add_assistant(full_response)\n",
    "    \n",
    "    return convo.messages\n",
    "\n",
    "# Run interactive chat (uncomment to use)\n",
    "# await interactive_chat(\n",
    "#     system_prompt=\"You are a friendly coding assistant.\",\n",
    "#     model=\"llama3.2\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f2c57",
   "metadata": {},
   "source": [
    "## 4. Effective System Prompts\n",
    "\n",
    "System prompts shape the assistant's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ee839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different personas with system prompts\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"coder\": \"\"\"You are an expert programmer. \n",
    "- Always provide code examples\n",
    "- Explain concepts clearly\n",
    "- Follow best practices\n",
    "- Consider edge cases\"\"\",\n",
    "    \n",
    "    \"teacher\": \"\"\"You are a patient teacher.\n",
    "- Break down complex topics\n",
    "- Use simple analogies\n",
    "- Ask guiding questions\n",
    "- Encourage learning\"\"\",\n",
    "    \n",
    "    \"reviewer\": \"\"\"You are a code reviewer.\n",
    "- Identify potential bugs\n",
    "- Suggest improvements\n",
    "- Check security issues\n",
    "- Be constructive\"\"\",\n",
    "    \n",
    "    \"creative\": \"\"\"You are a creative writer.\n",
    "- Be imaginative\n",
    "- Use vivid descriptions\n",
    "- Tell engaging stories\n",
    "- Have fun with ideas\"\"\"\n",
    "}\n",
    "\n",
    "# Test different personas\n",
    "query = \"What is recursion?\"\n",
    "\n",
    "for persona, prompt in list(SYSTEM_PROMPTS.items())[:2]:  # Test first 2\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Persona: {persona}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    response = await llm.chat(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(response.content[:500] + \"...\" if len(response.content) > 500 else response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00516f94",
   "metadata": {},
   "source": [
    "## 5. Structured Output\n",
    "\n",
    "Guide the model to return structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Request JSON output\n",
    "prompt = \"\"\"Analyze this text and return JSON with sentiment and keywords.\n",
    "\n",
    "Text: \"I absolutely loved this product! It works amazingly well and \n",
    "the customer service was fantastic. Highly recommend to everyone.\"\n",
    "\n",
    "Return ONLY valid JSON with this structure:\n",
    "{\n",
    "  \"sentiment\": \"positive\" | \"negative\" | \"neutral\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"keywords\": [\"keyword1\", \"keyword2\"],\n",
    "  \"summary\": \"brief summary\"\n",
    "}\"\"\"\n",
    "\n",
    "response = await llm.chat(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    options={\"temperature\": 0.1}  # Lower temperature for consistency\n",
    ")\n",
    "\n",
    "print(\"Raw response:\")\n",
    "print(response.content)\n",
    "\n",
    "# Parse JSON\n",
    "try:\n",
    "    # Try to extract JSON from response\n",
    "    content = response.content\n",
    "    if \"```json\" in content:\n",
    "        content = content.split(\"```json\")[1].split(\"```\")[0]\n",
    "    elif \"```\" in content:\n",
    "        content = content.split(\"```\")[1].split(\"```\")[0]\n",
    "    \n",
    "    data = json.loads(content.strip())\n",
    "    print(\"\\nParsed JSON:\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nCouldn't parse JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b3dad",
   "metadata": {},
   "source": [
    "## 6. Multiple Providers\n",
    "\n",
    "Switch between different LLM providers seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427aceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.llm import LLMClient, LLMProvider\n",
    "\n",
    "# Create clients for different providers\n",
    "clients = {}\n",
    "\n",
    "# Ollama (local)\n",
    "try:\n",
    "    clients[\"ollama\"] = LLMClient(provider=LLMProvider.OLLAMA)\n",
    "    print(\"âœ“ Ollama client ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Ollama unavailable: {e}\")\n",
    "\n",
    "# OpenAI (requires API key)\n",
    "try:\n",
    "    clients[\"openai\"] = LLMClient(provider=LLMProvider.OPENAI)\n",
    "    print(\"âœ“ OpenAI client ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— OpenAI unavailable: {e}\")\n",
    "\n",
    "# Anthropic (requires API key)\n",
    "try:\n",
    "    clients[\"anthropic\"] = LLMClient(provider=LLMProvider.ANTHROPIC)\n",
    "    print(\"âœ“ Anthropic client ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âœ— Anthropic unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f61e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses across providers\n",
    "prompt = \"Explain machine learning in 2 sentences.\"\n",
    "\n",
    "for name, client in clients.items():\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    try:\n",
    "        response = await client.chat(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        print(response.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310d1c6",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Great work! You've learned to build chat applications.\n",
    "\n",
    "Continue with:\n",
    "- **[03_rag_basics.ipynb](03_rag_basics.ipynb)** - Add document knowledge to your chats\n",
    "- **[04_advanced_prompting.ipynb](04_advanced_prompting.ipynb)** - Master prompt engineering\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Build a chatbot for a specific domain (cooking, fitness, etc.)\n",
    "2. Add conversation summarization for long chats\n",
    "3. Implement a multi-model chat that uses different models for different tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
